{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":396802,"sourceType":"datasetVersion","datasetId":175990}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\ncheckpoints_to_delete = [\n    \"/kaggle/working/fusion_best.pt\",\n    \"/kaggle/working/fusion_checkpoint.pt\"\n]\n\nfor checkpoint in checkpoints_to_delete:\n    if os.path.exists(checkpoint):\n        os.remove(checkpoint)\n        print(f\"Deleted: {checkpoint}\")\n    else:\n        print(f\"File not found: {checkpoint}\")\n\nprint(\"Cleanup complete! Ready for fresh training.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os\nimport sys\nimport math\nimport time\nimport json\nimport random\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torch.amp import autocast, GradScaler\n\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.models import resnet18, ResNet18_Weights\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# -----------------------\n# Config\n# -----------------------\nSEED = 42\nNUM_EPOCHS = 12\nBATCH_SIZE = 32\nNUM_WORKERS = 2\nIMAGE_SIZE = 224\nMAX_TFIDF_FEATURES = 5000\nTEXT_NGRAMS = (1, 3)  # unigrams to trigrams\nLR = 2e-4\nWEIGHT_DECAY = 1e-4\nLABEL_SMOOTHING = 0.1\nEARLY_STOP_PATIENCE = 4\nCHECKPOINT_PATH = \"/kaggle/working/fusion_checkpoint_v2.pt\"\nBEST_MODEL_PATH = \"/kaggle/working/fusion_best_v2.pt\"\nRUN_NAME = \"fusion_text_image_fashion_balanced_attention\"\nPCT_TRAIN = 0.8\nTARGET_COLUMN = \"masterCategory\"\nMIN_SAMPLES_PER_CLASS = 20  \nMC_DROPOUT_PASSES = 30\n\n# Focal Loss parameters\nFOCAL_ALPHA = 1.0\nFOCAL_GAMMA = 2.0\n\n# Attention parameters\nATTENTION_DIM = 256\nFUSION_DIM = 512\n\n# -----------------------\n# Utils\n# -----------------------\ndef seed_everything(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\nAMP_DEVICE_TYPE = 'cuda' if device.type == 'cuda' else 'cpu'\n\ndef find_fashion_dataset_root():\n    base = \"/kaggle/input\"\n    if not os.path.exists(base):\n        return None\n    candidates = []\n    for name in os.listdir(base):\n        path = os.path.join(base, name)\n        if not os.path.isdir(path):\n            continue\n        styles_here = os.path.exists(os.path.join(path, \"styles.csv\"))\n        images_here = os.path.isdir(os.path.join(path, \"images\"))\n        if styles_here and images_here:\n            return path\n        for sub in os.listdir(path):\n            subpath = os.path.join(path, sub)\n            if os.path.isdir(subpath):\n                if os.path.exists(os.path.join(subpath, \"styles.csv\")) and os.path.isdir(os.path.join(subpath, \"images\")):\n                    return subpath\n    return None\n\n# -----------------------\n# Focal Loss for Class Imbalance\n# -----------------------\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1.0, gamma=2.0, class_weights=None, label_smoothing=0.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.class_weights = class_weights\n        self.label_smoothing = label_smoothing\n        \n    def forward(self, inputs, targets):\n        # Standard cross entropy\n        ce_loss = F.cross_entropy(inputs, targets, weight=self.class_weights, \n                                 label_smoothing=self.label_smoothing, reduction='none')\n        \n        # Focal loss computation\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        \n        return focal_loss.mean()\n\n# -----------------------\n# Cross-Modal Attention Mechanism\n# -----------------------\nclass MultiHeadCrossAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n        super().__init__()\n        self.text_to_img = nn.MultiheadAttention(\n            embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True\n        )\n        self.img_to_text = nn.MultiheadAttention(\n            embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True\n        )\n        self.text_ln1 = nn.LayerNorm(d_model)\n        self.text_ln2 = nn.LayerNorm(d_model)\n        self.img_ln1 = nn.LayerNorm(d_model)\n        self.img_ln2 = nn.LayerNorm(d_model)\n        self.text_ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(4 * d_model, d_model),\n        )\n        self.img_ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(4 * d_model, d_model),\n        )\n\n    def forward(self, text_tokens: torch.Tensor, img_tokens: torch.Tensor):\n        # Text attends to image tokens (Q from text, K/V from image)\n        t2i_out, t2i_attn = self.text_to_img(\n            query=text_tokens,\n            key=img_tokens,\n            value=img_tokens,\n            need_weights=True,\n            average_attn_weights=True,\n        )  # t2i_attn: [B, T_text, T_img]\n        text_updated = self.text_ln1(text_tokens + t2i_out)\n        text_updated = self.text_ln2(text_updated + self.text_ffn(text_updated))\n\n        # Image attends to text tokens (optional symmetric co-attention)\n        i2t_out, _ = self.img_to_text(\n            query=img_tokens,\n            key=text_tokens,\n            value=text_tokens,\n            need_weights=False,\n        )\n        img_updated = self.img_ln1(img_tokens + i2t_out)\n        img_updated = self.img_ln2(img_updated + self.img_ffn(img_updated))\n\n        return text_updated, img_updated, t2i_attn\n\n# -----------------------\n# Enhanced Fusion Model with Attention\n# -----------------------\nclass AttentionFusionNet(nn.Module):\n    def __init__(self, num_text_features: int, num_classes: int):\n        super().__init__()\n\n        # Image backbone -> spatial tokens\n        backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n        self.image_cnn = nn.Sequential(*list(backbone.children())[:-2])  # up to layer4 output\n        self.img_channel_dim = 512\n        self.img_token_proj = nn.Conv2d(self.img_channel_dim, ATTENTION_DIM, kernel_size=1)\n\n        # Text branch - deeper network (produces a single token)\n        self.text_branch = nn.Sequential(\n            nn.Linear(num_text_features, 1024),\n            nn.ReLU(),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n        )\n        self.text_token_proj = nn.Linear(256, ATTENTION_DIM)\n\n        # Cross-modal multi-head attention\n        self.cross_attention = MultiHeadCrossAttention(\n            d_model=ATTENTION_DIM,\n            num_heads=8,\n            dropout=0.1,\n        )\n\n        # Fusion head\n        self.fusion_head = nn.Sequential(\n            nn.Linear(ATTENTION_DIM * 2, FUSION_DIM),\n            nn.ReLU(),\n            nn.BatchNorm1d(FUSION_DIM),\n            nn.Dropout(0.4),\n            nn.Linear(FUSION_DIM, FUSION_DIM // 2),\n            nn.ReLU(),\n            nn.BatchNorm1d(FUSION_DIM // 2),\n            nn.Dropout(0.3),\n            nn.Linear(FUSION_DIM // 2, num_classes),\n        )\n\n    def forward(self, images: torch.Tensor, text_vecs: torch.Tensor):\n        # Image -> tokens [B, T_img, D]\n        feats = self.image_cnn(images)  # [B, 512, H, W]\n        feats = self.img_token_proj(feats)  # [B, D, H, W]\n        B, D, H, W = feats.shape\n        img_tokens = feats.flatten(2).transpose(1, 2)  # [B, H*W, D]\n\n        # Text -> single token [B, 1, D]\n        text_feats = self.text_branch(text_vecs)  # [B, 256]\n        text_tokens = self.text_token_proj(text_feats).unsqueeze(1)  # [B, 1, D]\n\n        # Cross-attention\n        text_out, img_out_tokens, t2i_attn = self.cross_attention(text_tokens, img_tokens)\n\n        # Pool image tokens and fuse\n        img_pooled = img_out_tokens.mean(dim=1)  # [B, D]\n        fused = torch.cat([img_pooled, text_out.squeeze(1)], dim=1)\n        logits = self.fusion_head(fused)\n\n        # Attention stats: normalized entropy of text->image attention\n        with torch.no_grad():\n            p = t2i_attn.float().clamp(min=1e-6)  # [B, 1, T_img]\n            entropy = - (p * p.log()).sum(dim=-1) / math.log(p.size(-1))  # [B, 1]\n            t2i_entropy = entropy.mean()  # scalar tensor\n        attention_stats = {\"t2i_entropy\": t2i_entropy}\n\n        return logits, attention_stats\n\n# -----------------------\n# Data Augmentation for Minority Classes\n# -----------------------\nclass BalancedFashionDataset(Dataset):\n    def __init__(self, frame: pd.DataFrame, text_csr, transform, text_vectorizer_vocab_size: int, \n                 oversample_minority=True, target_samples_per_class=None):\n        self.original_frame = frame.reset_index(drop=True)\n        self.text_csr = text_csr\n        self.transform = transform\n        self.num_text_features = text_vectorizer_vocab_size\n        self.oversample_minority = oversample_minority\n        \n        if oversample_minority and target_samples_per_class is not None:\n            self.frame, self.augmented_text_csr = self._oversample_minority_classes(target_samples_per_class)\n        else:\n            self.frame = self.original_frame\n            self.augmented_text_csr = text_csr\n\n    def _oversample_minority_classes(self, target_samples):\n        \"\"\"Simple oversampling by repeating minority class samples\"\"\"\n        class_counts = self.original_frame['label_idx'].value_counts()\n        augmented_frames = []\n        augmented_texts = []\n        \n        for class_idx in class_counts.index:\n            class_frame = self.original_frame[self.original_frame['label_idx'] == class_idx]\n            class_text = self.text_csr[class_frame.index]\n            current_count = len(class_frame)\n            \n            if current_count < target_samples:\n                # Oversample this class\n                repeat_factor = math.ceil(target_samples / current_count)\n                repeated_indices = np.tile(class_frame.index.values, repeat_factor)[:target_samples]\n                \n                oversampled_frame = self.original_frame.loc[repeated_indices].reset_index(drop=True)\n                oversampled_text = self.text_csr[repeated_indices]\n                \n                augmented_frames.append(oversampled_frame)\n                augmented_texts.append(oversampled_text)\n            else:\n                augmented_frames.append(class_frame.reset_index(drop=True))\n                augmented_texts.append(class_text)\n        \n        final_frame = pd.concat(augmented_frames, ignore_index=True)\n        \n        # Stack text matrices\n        from scipy.sparse import vstack\n        final_text = vstack(augmented_texts)\n        \n        return final_frame, final_text\n\n    def __len__(self):\n        return len(self.frame)\n\n    def __getitem__(self, idx: int):\n        row = self.frame.iloc[idx]\n        \n        # Image with enhanced augmentation for minority classes\n        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n        img = self.transform(img)\n        \n        # Text vector\n        x_text_dense = self.augmented_text_csr[idx].toarray().astype(np.float32).squeeze(0)\n        x_text = torch.from_numpy(x_text_dense)\n        \n        label = int(row[\"label_idx\"])\n        \n        return img, x_text, label, row[\"productDisplayName\"], row[\"image_path\"]\n\n# -----------------------\n# Load and prepare data\n# -----------------------\nDATASET_ROOT = find_fashion_dataset_root()\nif DATASET_ROOT is None:\n    raise RuntimeError(\"Could not find dataset. Please add the Kaggle dataset as input.\")\nprint(f\"Using dataset at: {DATASET_ROOT}\")\n\nstyles_path = os.path.join(DATASET_ROOT, \"styles.csv\")\ndf = pd.read_csv(styles_path, on_bad_lines=\"skip\")\n\ndef resolve_image_path(row_id):\n    for ext in (\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\"):\n        candidate = os.path.join(DATASET_ROOT, \"images\", f\"{row_id}{ext}\")\n        if os.path.exists(candidate):\n            return candidate\n    return None\n\ndf[\"image_path\"] = df[\"id\"].apply(resolve_image_path)\ndf = df.dropna(subset=[\"image_path\", \"productDisplayName\", TARGET_COLUMN]).copy()\n\n# Handle extreme class imbalance - remove classes with very few samples\nclass_counts = df[TARGET_COLUMN].value_counts()\nprint(\"Original class distribution:\")\nfor class_name, count in class_counts.items():\n    print(f\"  {class_name}: {count}\")\n\n# Remove extremely small classes (they cause training instability)\nEXTREME_MIN_SAMPLES = 200  # Increase threshold significantly\nvalid_classes = class_counts[class_counts >= EXTREME_MIN_SAMPLES].index.tolist()\nremoved_classes = class_counts[class_counts < EXTREME_MIN_SAMPLES].index.tolist()\n\nif removed_classes:\n    print(f\"\\nRemoving classes with < {EXTREME_MIN_SAMPLES} samples: {removed_classes}\")\n\ndf = df[df[TARGET_COLUMN].isin(valid_classes)].copy()\n\n# Encode labels\nclasses = sorted(df[TARGET_COLUMN].unique().tolist())\nclass_to_idx = {c: i for i, c in enumerate(classes)}\nidx_to_class = {i: c for c, i in class_to_idx.items()}\ndf[\"label_idx\"] = df[TARGET_COLUMN].map(class_to_idx).astype(int)\n\nprint(f\"Num samples: {len(df)} | Num classes: {len(classes)}\")\nprint(\"Class distribution:\")\nfor class_name, count in class_counts[valid_classes].items():\n    print(f\"  {class_name}: {count}\")\n\n# Train/val split\ntrain_df, val_df = train_test_split(\n    df,\n    test_size=1 - PCT_TRAIN,\n    random_state=SEED,\n    stratify=df[\"label_idx\"]\n)\n\n# Compute class weights for focal loss\nclass_weights = compute_class_weight(\n    'balanced',\n    classes=np.unique(train_df['label_idx']),\n    y=train_df['label_idx']\n)\nclass_weights_tensor = torch.FloatTensor(class_weights).to(device)\nprint(\"Class weights:\", class_weights)\n\n# Text processing\ndef normalize_text(s):\n    if not isinstance(s, str):\n        return \"\"\n    return s.lower().strip()\n\ntrain_texts = train_df[\"productDisplayName\"].fillna(\"\").apply(normalize_text).tolist()\nval_texts = val_df[\"productDisplayName\"].fillna(\"\").apply(normalize_text).tolist()\n\nvectorizer = TfidfVectorizer(\n    max_features=MAX_TFIDF_FEATURES,\n    ngram_range=TEXT_NGRAMS,\n    stop_words=\"english\",\n    min_df=2,\n    max_df=0.95\n)\nX_train_text = vectorizer.fit_transform(train_texts)\nX_val_text = vectorizer.transform(val_texts)\n\n# Enhanced transforms\ntrain_transform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE + 64, IMAGE_SIZE + 64)),\n    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.6, 1.0), ratio=(0.7, 1.4)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.RandomErasing(p=0.3, scale=(0.02, 0.2), value=\"random\"),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.CenterCrop(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Calculate reasonable target samples per class for oversampling\ntarget_samples_per_class = min(5000, max(class_counts[valid_classes]) // 3)  # Cap at 5000, use 1/3 of majority\n\n# Datasets with oversampling\ntrain_ds = BalancedFashionDataset(\n    train_df, X_train_text, train_transform, X_train_text.shape[1],\n    oversample_minority=True, target_samples_per_class=target_samples_per_class\n)\n\nval_ds = BalancedFashionDataset(\n    val_df, X_val_text, val_transform, X_val_text.shape[1],\n    oversample_minority=False\n)\n\nprint(f\"Training samples after balancing: {len(train_ds)}\")\nprint(f\"Validation samples: {len(val_ds)}\")\n\n# Use simpler, more stable data loading without weighted sampling\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=True,  # Simple shuffle instead of weighted sampling\n    num_workers=NUM_WORKERS,\n    pin_memory=True,\n    drop_last=True,\n)\n\nval_loader = DataLoader(\n    val_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True,\n    drop_last=False,\n)\n\n# -----------------------\n# Model Setup\n# -----------------------\nnum_classes = len(classes)\nmodel = AttentionFusionNet(\n    num_text_features=X_train_text.shape[1], \n    num_classes=num_classes\n).to(device)\n\n# Focal loss with class weights\ncriterion = FocalLoss(\n    alpha=FOCAL_ALPHA,\n    gamma=FOCAL_GAMMA,\n    class_weights=class_weights_tensor,\n    label_smoothing=LABEL_SMOOTHING\n)\n\noptimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscaler = GradScaler(AMP_DEVICE_TYPE)\n\n# Cosine annealing with warm restarts\nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=max(1, len(train_loader) * 2), T_mult=2, eta_min=LR * 0.1\n)\n\nstart_epoch = 0\nbest_val_f1 = -1.0\n\n# -----------------------\n# Training Functions\n# -----------------------\ndef train_one_epoch(epoch: int):\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_targets = []\n    attn_entropies = []\n\n    for step, batch in enumerate(train_loader):\n        images, text_vecs, labels, _, _ = batch\n        images = images.to(device, non_blocking=True)\n        text_vecs = text_vecs.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n\n        optimizer.zero_grad(set_to_none=True)\n\n        with autocast(device_type=AMP_DEVICE_TYPE, enabled=(AMP_DEVICE_TYPE == 'cuda')):\n            logits, attention_stats = model(images, text_vecs)\n            loss = criterion(logits, labels)\n\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        total_loss += loss.item() * images.size(0)\n        preds = torch.argmax(logits.detach(), dim=1).cpu().numpy().tolist()\n        all_preds.extend(preds)\n        all_targets.extend(labels.detach().cpu().numpy().tolist())\n\n        t2i_entropy_val = attention_stats[\"t2i_entropy\"]\n        if isinstance(t2i_entropy_val, torch.Tensor):\n            t2i_entropy_val = float(t2i_entropy_val.detach().cpu().item())\n        attn_entropies.append(t2i_entropy_val)\n\n        if (step + 1) % 100 == 0:\n            avg_entropy = float(np.mean(attn_entropies)) if len(attn_entropies) > 0 else float('nan')\n            print(\n                f\"Epoch {epoch} | Step {step+1}/{len(train_loader)} | Loss {loss.item():.4f} | \"\n                f\"t2i_attn_entropy {avg_entropy:.3f}\"\n            )\n\n    avg_loss = total_loss / len(train_ds)\n    acc = accuracy_score(all_targets, all_preds)\n    f1 = f1_score(all_targets, all_preds, average=\"macro\")\n    avg_attn_entropy = float(np.mean(attn_entropies)) if len(attn_entropies) > 0 else float('nan')\n\n    return avg_loss, acc, f1, avg_attn_entropy\n\n@torch.no_grad()\ndef evaluate():\n    model.eval()\n    total_loss = 0.0\n    all_preds = []\n    all_targets = []\n    attn_entropies = []\n\n    for batch in val_loader:\n        images, text_vecs, labels, _, _ = batch\n        images = images.to(device, non_blocking=True)\n        text_vecs = text_vecs.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n\n        with autocast(device_type=AMP_DEVICE_TYPE, enabled=(AMP_DEVICE_TYPE == 'cuda')):\n            logits, attention_stats = model(images, text_vecs)\n            loss = criterion(logits, labels)\n\n        total_loss += loss.item() * images.size(0)\n        preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n        all_preds.extend(preds)\n        all_targets.extend(labels.cpu().numpy().tolist())\n\n        t2i_entropy_val = attention_stats[\"t2i_entropy\"]\n        if isinstance(t2i_entropy_val, torch.Tensor):\n            t2i_entropy_val = float(t2i_entropy_val.detach().cpu().item())\n        attn_entropies.append(t2i_entropy_val)\n\n    avg_loss = total_loss / len(val_ds)\n    acc = accuracy_score(all_targets, all_preds)\n    f1 = f1_score(all_targets, all_preds, average=\"macro\")\n    avg_attn_entropy = float(np.mean(attn_entropies)) if len(attn_entropies) > 0 else float('nan')\n\n    return avg_loss, acc, f1, np.array(all_preds), np.array(all_targets), avg_attn_entropy\n\n# -----------------------\n# Training Loop\n# -----------------------\nepochs_no_improve = 0\nhistory = []\n\nprint(\"Starting training with attention fusion and class balancing...\")\n\nfor epoch in range(start_epoch, NUM_EPOCHS):\n    t0 = time.time()\n    tr_loss, tr_acc, tr_f1, tr_attn_entropy = train_one_epoch(epoch)\n    va_loss, va_acc, va_f1, va_preds, va_tgts, va_attn_entropy = evaluate()\n\n    print(\n        f\"[Epoch {epoch}] \"\n        f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f} f1={tr_f1:.4f} | \"\n        f\"val_loss={va_loss:.4f} acc={va_acc:.4f} f1={va_f1:.4f} | \"\n        f\"time={(time.time()-t0):.1f}s\"\n    )\n\n    print(f\"Train t2i attention entropy: {tr_attn_entropy:.3f}\")\n    print(f\"Val t2i attention entropy: {va_attn_entropy:.3f}\")\n\n    # Save checkpoint\n    torch.save({\n        \"epoch\": epoch,\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict(),\n        \"scaler\": scaler.state_dict(),\n        \"best_val_f1\": best_val_f1,\n        \"classes\": classes,\n        \"class_to_idx\": class_to_idx,\n        \"idx_to_class\": idx_to_class,\n    }, CHECKPOINT_PATH)\n\n    # Save best model\n    if va_f1 > best_val_f1:\n        best_val_f1 = va_f1\n        epochs_no_improve = 0\n        torch.save({\n            \"model\": model.state_dict(),\n            \"best_val_f1\": best_val_f1,\n            \"classes\": classes,\n            \"class_to_idx\": class_to_idx,\n            \"idx_to_class\": idx_to_class,\n        }, BEST_MODEL_PATH)\n        print(f\"New best model saved with val_f1={best_val_f1:.4f}\")\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve >= EARLY_STOP_PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\n# -----------------------\n# Final Evaluation\n# -----------------------\nif os.path.exists(BEST_MODEL_PATH):\n    ckpt = torch.load(BEST_MODEL_PATH, map_location=device, weights_only=False)\n    model.load_state_dict(ckpt[\"model\"])\n\nval_loss, val_acc, val_f1, val_preds, val_tgts, val_attn_entropy = evaluate()\nprint(f\"\\nFinal Results:\")\nprint(f\"Validation: loss={val_loss:.4f} acc={val_acc:.4f} f1={val_f1:.4f}\")\nprint(f\"Final t2i attention entropy: {val_attn_entropy:.3f}\")\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(val_tgts, val_preds, target_names=classes))\n\n# Confusion matrix\ncm = confusion_matrix(val_tgts, val_preds)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\nprint(\"\\nTraining completed with attention-based fusion and class balancing!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T06:33:46.065828Z","iopub.execute_input":"2025-08-21T06:33:46.066135Z","iopub.status.idle":"2025-08-21T07:02:11.383448Z","shell.execute_reply.started":"2025-08-21T06:33:46.066112Z","shell.execute_reply":"2025-08-21T07:02:11.382357Z"}},"outputs":[],"execution_count":null}]}